{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-nightly in c:\\users\\aslan\\appdata\\roaming\\python\\python37\\site-packages (2.4.0.dev20200808)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (3.12.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.1.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.27.2)\n",
      "Requirement already satisfied: tb-nightly<3.0.0a0,>=2.4.0a0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (2.4.0a20200808)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (0.3.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.18.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (0.34.2)\n",
      "Requirement already satisfied: tf-estimator-nightly in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (2.4.0.dev2020080801)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (0.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tf-nightly) (1.11.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tf-nightly) (45.2.0.post20200210)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.20.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.25.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aslan\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 288\n",
    "img_width = 432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3979 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir_train = \"OneDrive/Escritorio/Practicas Frustacion Videojuegos/img_melspec_struct/train/\"\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1326 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir_val = \"OneDrive/Escritorio/Practicas Frustacion Videojuegos/img_melspec_struct/evaluate/\"\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_val,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1328 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir_test = \"OneDrive/Escritorio/Practicas Frustacion Videojuegos/img_melspec_struct/test/\"\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_test,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 288, 432, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_1 (Rescaling)      (None, 288, 432, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 288, 432, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 144, 216, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 144, 216, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 72, 108, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 108, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 124416)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               15925376  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 15,949,218\n",
      "Trainable params: 15,949,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.793975903614458, 1: 0.5582210998877666}\n"
     ]
    }
   ],
   "source": [
    "#weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "# recordemos que tenemos un batch_size=32 por lo que tenemos dividios los spectrogramas de 32 en 32, aunque eb \n",
    "# las ultimas iteraciones no tiene porque pasar esto, por lo que utilizamos len()\n",
    "y_train = []\n",
    "for i in train_ds:\n",
    "    for count_batch in range(0,len(i.__getitem__(1).numpy())):\n",
    "        #print(i.__getitem__(1))\n",
    "        y_train.append(i.__getitem__(1).numpy()[count_batch])\n",
    "\n",
    "my_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights = {0: my_weights[0], 1: my_weights[1]}\n",
    "print(class_weights)\n",
    "#callbacks\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model.{epoch:02d}.h5', monitor='val_acc', verbose=0, save_best_only=False,\n",
    "        save_weights_only=False, mode='max', save_freq='epoch', options=None)\n",
    "]\n",
    "#epochs\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback.\n",
      "Epoch 1/100\n",
      "125/125 [==============================] - 232s 2s/step - loss: 0.3889 - accuracy: 0.8082 - val_loss: 0.4721 - val_accuracy: 0.7647\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 240s 2s/step - loss: 0.3045 - accuracy: 0.8537 - val_loss: 0.3302 - val_accuracy: 0.8635\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 229s 2s/step - loss: 0.2537 - accuracy: 0.8748 - val_loss: 0.4124 - val_accuracy: 0.8318\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 235s 2s/step - loss: 0.2066 - accuracy: 0.9002 - val_loss: 0.3468 - val_accuracy: 0.8906\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 228s 2s/step - loss: 0.1774 - accuracy: 0.9196 - val_loss: 0.4086 - val_accuracy: 0.8462\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 237s 2s/step - loss: 0.1311 - accuracy: 0.9394 - val_loss: 0.4579 - val_accuracy: 0.9087\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 228s 2s/step - loss: 0.1256 - accuracy: 0.9447 - val_loss: 0.4127 - val_accuracy: 0.8643\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 267s 2s/step - loss: 0.0953 - accuracy: 0.9605 - val_loss: 0.4367 - val_accuracy: 0.9012\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 248s 2s/step - loss: 0.0832 - accuracy: 0.9663 - val_loss: 0.4647 - val_accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 231s 2s/step - loss: 0.0604 - accuracy: 0.9749 - val_loss: 0.5229 - val_accuracy: 0.8876\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 228s 2s/step - loss: 0.0654 - accuracy: 0.9744 - val_loss: 0.5040 - val_accuracy: 0.8771\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 232s 2s/step - loss: 0.0456 - accuracy: 0.9827 - val_loss: 0.5486 - val_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  class_weight=class_weights,\n",
    "  callbacks=my_callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 21s 488ms/step - loss: 0.5553 - accuracy: 0.0896\n",
      "Test Accuracy2: 0.08960843086242676\n",
      "Test Lost2: 0.555250883102417\n"
     ]
    }
   ],
   "source": [
    "#PROBAMOS A COGER UN MODELO GUARDADO\n",
    "model_new = tf.keras.models.load_model('model.12.h5')\n",
    "\n",
    "test_lost, test_acc= model_new.evaluate(test_ds)\n",
    "print (\"Test Accuracy2:\", test_acc)\n",
    "print (\"Test Lost2:\", test_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 577ms/step - loss: 0.5553 - accuracy: 0.8931\n",
      "Test Accuracy: 0.8930723071098328\n",
      "Test Loss: 0.5552509427070618\n"
     ]
    }
   ],
   "source": [
    "# PROBAMOS CON EL ULTIMO MODELO CARGADO\n",
    "test_loss, test_acc= model.evaluate(test_ds)\n",
    "print (\"Test Accuracy:\", test_acc)\n",
    "print (\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por ultimo comprobamos nuestro porcentaje de acierto\n",
    "test = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328\n"
     ]
    }
   ],
   "source": [
    "classes = np.argmax(test, axis = 1)\n",
    "print(len(classes))\n",
    "#for i in classes:\n",
    "#    print(i)\n",
    "#print(classes[62]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# recordemos que tenemos un batch_size=32 por lo que tenemos dividios los spectrogramas de 32 en 32, aunque eb \n",
    "# las ultimas iteraciones no tiene porque pasar esto, por lo que utilizamos len()\n",
    "classes_predictions = []\n",
    "for i in test_ds:\n",
    "    for count_batch in range(0,len(i.__getitem__(1).numpy())):\n",
    "        #print(i.__getitem__(1))\n",
    "        classes_predictions.append(i.__getitem__(1).numpy()[count_batch])\n",
    "            \n",
    "print(len(classes_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 1328\n",
      "Acertadas: 1100\n",
      "Falladas: 228\n",
      "Acertadas Frustracion: 8\n",
      "Accuracy: 0.8283132530120482\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "count_wrong = 0\n",
    "countCorrectFrus = 0\n",
    "for i in range(0,len(classes)):\n",
    "    #vamos a crear un csv\n",
    "    if classes[i] == 0 and classes_predictions[i] == 0:\n",
    "        countCorrectFrus = countCorrectFrus+1\n",
    "    if classes[i] != classes_predictions[i]:\n",
    "        count_wrong = count_wrong+1\n",
    "    else:\n",
    "        count_correct = count_correct+1\n",
    "print(\"total:\", len(classes))\n",
    "print(\"Acertadas:\", count_correct)\n",
    "print(\"Falladas:\", count_wrong)\n",
    "print(\"Acertadas Frustracion:\",countCorrectFrus)\n",
    "\n",
    "# seria como el accuracy???\n",
    "print(\"Accuracy:\", count_correct/len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Label':classes,\n",
    "    'Predictions':classes_predictions})\n",
    "df.to_excel(\"img_melspec_predictions_weight.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
